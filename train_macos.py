#!/usr/bin/env python3
"""
macOSå…¼å®¹çš„ShowUIå¾®è°ƒè®­ç»ƒè„šæœ¬
ç§»é™¤äº†deepspeedä¾èµ–ï¼Œä½¿ç”¨æ ‡å‡†PyTorchè®­ç»ƒ
"""

import argparse
import os
import sys
import json

from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import LinearLR
import transformers
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoProcessor, BitsAndBytesConfig
from tqdm import tqdm
from PIL import Image

class ShowUIDataset(Dataset):
    """ShowUIæ•°æ®é›†ç±»"""
    def __init__(self, data_path, processor, args):
        self.data_path = data_path
        self.processor = processor
        self.args = args

        # è¯»å–æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            if data_path.endswith('.json'):
                # JSONæ ¼å¼
                self.data = json.load(f)
            else:
                # JSONLæ ¼å¼
                self.data = []
                for line in f:
                    if line.strip():
                        self.data.append(json.loads(line))

        print(f"ğŸ“Š åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]

        # æ ¹æ®æ•°æ®æ ¼å¼å¤„ç†
        if 'img_url' in item:
            # æ–°æ ¼å¼ï¼šmy_dataset/metadata.json
            image_filename = item['img_url']
            image_path = os.path.join(os.path.dirname(self.data_path), 'images', image_filename)

            # æ„å»ºè®­ç»ƒæ–‡æœ¬
            elements = item.get('element', [])
            if elements:
                element = elements[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
                instruction = element.get('instruction', 'ç‚¹å‡»')
                point = element.get('point', [0.5, 0.5])

                # è½¬æ¢ç›¸å¯¹åæ ‡åˆ°ç»å¯¹åæ ‡
                img_size = item.get('img_size', [1282, 846])
                abs_x = int(point[0] * img_size[0])
                abs_y = int(point[1] * img_size[1])

                text = f"ç”¨æˆ·: è¯·ç‚¹å‡»{instruction}\nåŠ©æ‰‹: æˆ‘ä¼šå¸®æ‚¨ç‚¹å‡»{instruction}ã€‚<click>{abs_x}, {abs_y}</click>"
            else:
                text = "ç”¨æˆ·: æè¿°è¿™ä¸ªå›¾ç‰‡\nåŠ©æ‰‹: è¿™æ˜¯ä¸€ä¸ªç•Œé¢æˆªå›¾ã€‚"

        else:
            # æ—§æ ¼å¼ï¼šconversations
            image_path = os.path.join(os.path.dirname(self.data_path), item['image'])
            conversations = item['conversations']

            # æ„å»ºç®€å•çš„å¯¹è¯æ–‡æœ¬
            text = ""
            for conv in conversations:
                if conv['from'] == 'human':
                    user_text = conv['value'].replace('<image>\n', '').replace('<image>', '').strip()
                    text += f"ç”¨æˆ·: {user_text}\n"
                elif conv['from'] == 'gpt' or conv['from'] == 'assistant':
                    assistant_text = conv['value'].strip()
                    text += f"åŠ©æ‰‹: {assistant_text}\n"

        # åŠ è½½å›¾ç‰‡
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾ç‰‡ {image_path}: {e}")
            # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾ç‰‡ä½œä¸ºfallback
            image = Image.new('RGB', (224, 224), color='white')

        # ä½¿ç”¨processorå¤„ç†ï¼ˆæš‚æ—¶åªç”¨æ–‡æœ¬ï¼Œé¿å…å›¾åƒtokené—®é¢˜ï¼‰
        try:
            # æš‚æ—¶åªä½¿ç”¨æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œé¿å…å›¾åƒtokenåŒ¹é…é—®é¢˜
            inputs = self.processor.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.args.model_max_length
            )

            # è®¾ç½®labels
            inputs["labels"] = inputs["input_ids"].clone()

            # å°†tensorä»batchç»´åº¦ä¸­å–å‡º
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor) and inputs[key].dim() > 1:
                    inputs[key] = inputs[key].squeeze(0)

            return inputs
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªç®€å•çš„fallback
            return {
                "input_ids": torch.zeros(100, dtype=torch.long),
                "attention_mask": torch.ones(100, dtype=torch.long),
                "labels": torch.zeros(100, dtype=torch.long)
            }

def parse_args():
    parser = argparse.ArgumentParser(description="ShowUIè®­ç»ƒ - macOSå…¼å®¹ç‰ˆæœ¬")
    
    # åŸºç¡€å‚æ•°

    parser.add_argument("--precision", default="fp16", type=str, choices=["fp32", "bf16", "fp16"])
    parser.add_argument("--use_qlora", action="store_true", default=True)
    parser.add_argument("--load_in_4bit", action="store_true", default=True)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_id", default="showlab/ShowUI-2B")
    parser.add_argument("--local_weight", action="store_true", default=True)
    parser.add_argument("--local_weight_dir", default="./models", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--dataset_dir", default="./data", type=str)
    parser.add_argument("--train_json", default="metadata.jsonl", type=str)
    parser.add_argument("--model_max_length", default=2048, type=int)
    
    # LoRAå‚æ•°
    parser.add_argument("--lora_r", default=16, type=int)
    parser.add_argument("--lora_alpha", default=32, type=int)
    parser.add_argument("--lora_dropout", default=0.1, type=float)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--log_base_dir", default="./logs", type=str)
    parser.add_argument("--exp_id", default="showui_macos", type=str)
    parser.add_argument("--epochs", default=1, type=int)
    parser.add_argument("--max_steps", default=None, type=int, help="æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œå¦‚æœè®¾ç½®åˆ™è¦†ç›–epochs")
    parser.add_argument("--lr", default=2e-4, type=float)
    parser.add_argument("--batch_size", default=1, type=int)
    parser.add_argument("--grad_accumulation_steps", default=4, type=int)
    parser.add_argument("--warmup_steps", default=50, type=int)
    parser.add_argument("--print_freq", default=5, type=int)
    parser.add_argument("--save_steps", default=100, type=int)
    
    return parser.parse_args()

def setup_model_and_processor(args):
    """è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨"""
    print("ğŸ”§ æ­£åœ¨è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨...")
    
    # ç¡®å®šè®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ ä½¿ç”¨MPS (Metal Performance Shaders)")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("ğŸš€ ä½¿ç”¨CUDA")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨CPU")
    
    # è®¾ç½®æ•°æ®ç±»å‹
    torch_dtype = torch.float32
    if args.precision == "bf16" and device.type != "mps":  # MPSä¸æ”¯æŒbf16
        torch_dtype = torch.bfloat16
    elif args.precision == "fp16":
        torch_dtype = torch.half
    
    # é‡åŒ–é…ç½®
    bnb_config = None
    if args.use_qlora and args.load_in_4bit and device.type != "mps":  # MPSä¸æ”¯æŒé‡åŒ–
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch_dtype,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
    
    # åŠ è½½å¤„ç†å™¨
    try:
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            trust_remote_code=True
        )
        print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
        return None, None, None
    
    # åŠ è½½æ¨¡å‹
    try:
        from transformers import Qwen2VLForConditionalGeneration
        
        model_path = f"{args.local_weight_dir}/ShowUI-2B"
        
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch_dtype,
            device_map="auto" if device.type == "cuda" else None,
            quantization_config=bnb_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if device.type == "mps":
            model = model.to(device)
            
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ShowUI-2Bæ¨¡å‹æ–‡ä»¶åœ¨ ./models/ShowUI-2B ç›®å½•ä¸‹")
        return None, None, None
    
    return model, processor, device

def setup_lora(model, args):
    """è®¾ç½®LoRAå¾®è°ƒ"""
    if args.lora_r <= 0:
        return model
        
    print("ğŸ”§ æ­£åœ¨è®¾ç½®LoRA...")
    
    if args.use_qlora:
        model = prepare_model_for_kbit_training(model)
    
    # ç›®æ ‡æ¨¡å—
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=target_modules,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model

def train_epoch(model, dataloader, optimizer, scheduler, device, args, epoch, global_step=0):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    step_count = 0

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.epochs}")

    for step, batch in enumerate(progress_bar):
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
        if args.max_steps and global_step >= args.max_steps:
            print(f"ğŸ¯ è¾¾åˆ°æœ€å¤§æ­¥æ•° {args.max_steps}ï¼Œåœæ­¢è®­ç»ƒ")
            break
        try:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            if device.type != "cpu":
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(**batch)
            loss = outputs.loss / args.grad_accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            if (step + 1) % args.grad_accumulation_steps == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                step_count += 1
                global_step += 1

            total_loss += loss.item() * args.grad_accumulation_steps

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item() * args.grad_accumulation_steps:.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.2e}',
                'step': f'{global_step}'
            })



        except Exception as e:
            print(f"âš ï¸ è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
            continue

    return total_loss / max(len(dataloader), 1), global_step

def main():
    args = parse_args()
    
    print("ğŸš€ å¼€å§‹ShowUIå¾®è°ƒè®­ç»ƒ (macOSç‰ˆæœ¬)")
    print(f"ğŸ“± æ¨¡å‹: {args.model_id}")
    print(f"ğŸ¯ å®éªŒ: {args.exp_id}")
    

    
    # è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨
    model, processor, device = setup_model_and_processor(args)
    if model is None:
        print("âŒ æ¨¡å‹è®¾ç½®å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # è®¾ç½®LoRA
    model = setup_lora(model, args)
    
    # åˆ›å»ºæ•°æ®é›†
    train_data_path = os.path.join(args.dataset_dir, args.train_json)
    dataset = ShowUIDataset(train_data_path, processor, args)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
    
    # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=args.lr)
    total_steps = args.epochs * len(dataloader) // args.grad_accumulation_steps
    scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=args.warmup_steps)
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    global_step = 0

    for epoch in range(args.epochs):
        avg_loss, global_step = train_epoch(model, dataloader, optimizer, scheduler, device, args, epoch, global_step)
        print(f"Epoch {epoch+1}/{args.epochs} - å¹³å‡æŸå¤±: {avg_loss:.4f} - æ€»æ­¥æ•°: {global_step}")



        # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œæå‰ç»“æŸ
        if args.max_steps and global_step >= args.max_steps:
            print(f"ğŸ¯ è¾¾åˆ°æœ€å¤§æ­¥æ•° {args.max_steps}ï¼Œè®­ç»ƒç»“æŸ")
            break
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    
    # ä¿å­˜æ¨¡å‹
    save_path = f"{args.log_base_dir}/{args.exp_id}"
    os.makedirs(save_path, exist_ok=True)
    model.save_pretrained(save_path)
    print(f"ğŸ’¾ LoRAæƒé‡å·²ä¿å­˜åˆ° {save_path}")

if __name__ == "__main__":
    main()
