#!/usr/bin/env python3
"""
ä¸€ä¸ªé€šç”¨çš„ã€é…ç½®é©±åŠ¨çš„Hugging Faceæ¨¡å‹å¾®è°ƒè„šæœ¬ã€‚
æ”¯æŒå¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚ShowUI-2Bï¼‰å’Œçº¯æ–‡æœ¬æ¨¡å‹ã€‚
æ”¯æŒQLoRAå’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚
"""
import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    AutoProcessor,
    AutoModelForCausalLM,
    AutoTokenizer,
    get_linear_schedule_with_warmup,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from tqdm import tqdm
from PIL import Image
from datetime import datetime
import pprint


# ========================================================================================
# 1. é…ç½®ä¸­å¿ƒ (Config): è¿™æ˜¯ä½ å”¯ä¸€éœ€è¦ä¿®æ”¹çš„åœ°æ–¹ï¼
#    æˆ‘ä»¬ä»¥å¾®è°ƒ ShowUI-2B ä¸ºä¾‹æ¥å¡«å……å®ƒã€‚
# ========================================================================================
class Config:
    """
    é€šç”¨å¾®è°ƒé…ç½®ç±»ã€‚
    ä¿®æ”¹è¿™é‡Œçš„å‚æ•°ä»¥é€‚åº”ä½ çš„æ¨¡å‹å’Œæ•°æ®ã€‚
    """
    # --- [ä¾¦æŸ¥æ­¥éª¤1: æ¨¡å‹èº«ä»½] ---
    # Hugging Faceæ¨¡å‹ID
    MODEL_ID = "showlab/ShowUI-2B"
    # æœ¬åœ°æ¨¡å‹æƒé‡çš„æ ¹ç›®å½•
    LOCAL_MODEL_DIR = "./models"
    # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç  (å¯¹äºè‡ªå®šä¹‰æ¨¡å‹æ¶æ„å¦‚Qwen-VL, Llavaæ˜¯å¿…é¡»çš„)
    TRUST_REMOTE_CODE = True
    # æ¨¡å‹ç±»å‹: 'vision' æˆ– 'text'ã€‚è¿™ä¼šå½±å“æ•°æ®å¤„ç†å’Œæ¨¡å‹åŠ è½½æ–¹å¼ã€‚
    MODEL_TYPE = "vision"  # 'vision' or 'text'

    # --- [ä¾¦æŸ¥æ­¥éª¤2: å¤„ç†å™¨ç‰¹æ®Šç™–å¥½] ---
    # è¿™ä¸ªå­—å…¸ä¼šè¢«ç›´æ¥ä¼ é€’ç»™ AutoProcessor.from_pretrained
    # å¯¹äºçº¯æ–‡æœ¬æ¨¡å‹ï¼Œè¿™ä¸ªå¯ä»¥ç•™ç©º {}
    PROCESSOR_KWARGS = {
        "min_pixels": 256 * 28 * 28,
        "max_pixels": 1344 * 28 * 28,
        "size": {'shortest_edge': 448, 'longest_edge': 448},
        "uigraph_train": True,  # ShowUI ç‰¹æœ‰å‚æ•°
    }

    # --- [ä¾¦æŸ¥æ­¥éª¤3: æ•°æ®æ ¼å¼ä¸å¯¹è¯æ¨¡æ¿] ---
    # æ•°æ®é›†æ ¹ç›®å½•
    DATASET_DIR = "./data"
    # è®­ç»ƒæ•°æ®JSON/JSONLæ–‡ä»¶å
    TRAIN_JSON = "my_dataset/metadata.json"
    # å›¾ç‰‡æ–‡ä»¶å¤¹ç›¸å¯¹äºJSONæ–‡ä»¶çš„è·¯å¾„ (ä»…å¯¹è§†è§‰æ¨¡å‹æœ‰æ•ˆ)
    IMAGE_SUBDIR = "images"
    # å¦‚æœä¸ºNoneï¼Œåˆ™ä¾èµ–processorè‡ªåŠ¨åŠ è½½ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å¼ºåˆ¶è®¾ç½®ã€‚
    # å¯¹äºShowUI-2Bï¼Œæœ€å¥½æ‰‹åŠ¨è®¾ç½®ä»¥ç¡®ä¿ä¸€è‡´æ€§ã€‚
    CHAT_TEMPLATE = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"

    # --- [ä¾¦æŸ¥æ­¥éª¤4: LoRAé¶å¿ƒ] ---
    # LoRAç›®æ ‡æ¨¡å—ï¼Œé€šå¸¸éœ€è¦æ ¹æ®æ¨¡å‹æ£€æŸ¥æ¥ç¡®å®š
    LORA_TARGET_MODULES = []  # ç•™ç©ºï¼Œè§¦å‘è‡ªåŠ¨æ£€æµ‹ï¼

    # --- [è®­ç»ƒè¶…å‚æ•°] ---
    EXP_ID = f"finetune_{MODEL_ID.split('/')[-1]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    EPOCHS = 3
    LR = 2e-4
    BATCH_SIZE = 1
    GRAD_ACCUMULATION_STEPS = 8
    MODEL_MAX_LENGTH = 2048

    # --- [ç¡¬ä»¶ä¸æ€§èƒ½é…ç½®] ---
    USE_QLORA = True
    LORA_R = 16
    LORA_ALPHA = 32
    LORA_DROPOUT = 0.1
    PRECISION = "bf16"  # "bf16", "fp16", "fp32"

    # --- [æ•°æ®å¤„ç†æ ¸å¿ƒé€»è¾‘] ---
    # è¿™ä¸ªæ–¹æ³•å®šä¹‰äº†å¦‚ä½•ä»ä¸€ä¸ªJSON itemè½¬æ¢æˆæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼
    @staticmethod
    def get_model_inputs(item, processor, cfg):
        # --- é€‚ç”¨äº ShowUI-2B / Qwen-VL çš„é€»è¾‘ ---
        if cfg.MODEL_TYPE == 'vision':
            image_path = os.path.join(cfg.DATASET_DIR, os.path.dirname(cfg.TRAIN_JSON), cfg.IMAGE_SUBDIR,
                                      item['img_url'])
            instruction = item['element'][0]['instruction']
            point = str(item['element'][0]['point'])
            system_prompt = "Based on the screenshot of the page, I give a text description and you give its corresponding location..."

            messages = [
                {"role": "user", "content": [{"type": "text", "text": system_prompt}, {"type": "image"},
                                             {"type": "text", "text": instruction}]},
                {"role": "assistant", "content": point}
            ]

            text = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            image = Image.open(image_path).convert('RGB')

            image_inputs_dict = processor.image_processor(images=image, return_tensors="pt")
            text_inputs = processor.tokenizer(text, return_tensors="pt", truncation=True,
                                              max_length=cfg.MODEL_MAX_LENGTH)

            inputs = {**text_inputs, **image_inputs_dict}
            inputs["labels"] = inputs["input_ids"].clone()
            return inputs

        # --- é€‚ç”¨äºçº¯æ–‡æœ¬æ¨¡å‹çš„é€»è¾‘ (ä¾‹å¦‚: Llama, Mistral) ---
        elif cfg.MODEL_TYPE == 'text':
            # å‡è®¾ä½ çš„JSONæ˜¯ { "instruction": "...", "input": "...", "output": "..." } æ ¼å¼
            instruction = item.get('instruction', '')
            input_text = item.get('input', '')
            output_text = item.get('output', '')

            # ä½¿ç”¨ Alpaca æ ¼å¼çš„æ¨¡æ¿
            if input_text:
                prompt = f"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n"
            else:
                prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n"

            full_text = prompt + output_text
            inputs = processor(full_text, return_tensors="pt", truncation=True, max_length=cfg.MODEL_MAX_LENGTH)
            inputs["labels"] = inputs["input_ids"].clone()
            return inputs

        else:
            raise ValueError(f"Unsupported MODEL_TYPE: {cfg.MODEL_TYPE}")


# ========================================================================================
# 2. æ•°æ®é›†ç±» (é€šç”¨ï¼Œæ— éœ€ä¿®æ”¹)
# ========================================================================================
class UniversalDataset(Dataset):
    def __init__(self, cfg, processor):
        self.cfg = cfg
        self.processor = processor
        data_path = os.path.join(cfg.DATASET_DIR, cfg.TRAIN_JSON)
        with open(data_path, 'r', encoding='utf-8') as f:
            # æ”¯æŒ .jsonl å’Œ .json
            if data_path.endswith('.jsonl'):
                self.data = [json.loads(line) for line in f if line.strip()]
            else:
                self.data = json.load(f)
        print(f"ğŸ“Š åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®ä» {data_path}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        try:
            inputs = self.cfg.get_model_inputs(self.data[idx], self.processor, self.cfg)
            # DataLoader ä¼šè‡ªåŠ¨æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§»é™¤å®ƒ
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor) and inputs[key].dim() > 1:
                    inputs[key] = inputs[key].squeeze(0)
            return inputs
        except Exception as e:
            print(f"âŒ å¤„ç†æ•°æ®æ—¶å‡ºé”™! Item index: {idx}. Error: {e}")
            return None


# ========================================================================================
# 3. è®­ç»ƒå™¨ (é€šç”¨ï¼Œæ— éœ€ä¿®æ”¹)
# ========================================================================================
class Trainer:
    def __init__(self, cfg):
        self.cfg = cfg
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._print_config()
        self.model, self.processor = self._setup_model_and_processor()

    def _print_config(self):
        print("=" * 60)
        print("ğŸš€ Universal Finetuner: Configuration Overview ğŸš€")
        print("=" * 60)
        config_dict = {k: v for k, v in self.cfg.__dict__.items() if not k.startswith('__') and not callable(v)}
        pprint.pprint(config_dict, indent=2, width=100)
        print("=" * 60)

    def _setup_model_and_processor(self):
        print("ğŸ”§ æ­£åœ¨è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨...")

        torch_dtype = {"bf16": torch.bfloat16, "fp16": torch.half, "fp32": torch.float32}[self.cfg.PRECISION]
        bnb_config = None
        if self.cfg.USE_QLORA:
            bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch_dtype,
                                            bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4")

        model_path = os.path.join(self.cfg.LOCAL_MODEL_DIR, self.cfg.MODEL_ID.split('/')[-1])
        if not os.path.exists(model_path):
            print(f"âš ï¸ æœ¬åœ°è·¯å¾„ {model_path} ä¸å­˜åœ¨ï¼Œå°†å°è¯•ä» Hub åŠ è½½ {self.cfg.MODEL_ID}")
            model_path = self.cfg.MODEL_ID

        if self.cfg.MODEL_TYPE == 'vision':
            processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=self.cfg.TRUST_REMOTE_CODE,
                                                      **self.cfg.PROCESSOR_KWARGS)
        else:  # text
            processor = AutoTokenizer.from_pretrained(model_path, trust_remote_code=self.cfg.TRUST_REMOTE_CODE,
                                                      use_fast=False)

        if self.cfg.CHAT_TEMPLATE:
            print("ğŸ‘ æ­£åœ¨æ‰‹åŠ¨è®¾ç½®èŠå¤©æ¨¡æ¿...")
            processor.chat_template = self.cfg.CHAT_TEMPLATE
            if hasattr(processor, 'tokenizer'):  # for vision model
                processor.tokenizer.chat_template = self.cfg.CHAT_TEMPLATE

        model = AutoModelForCausalLM.from_pretrained(
            model_path, torch_dtype=torch_dtype, quantization_config=bnb_config,
            trust_remote_code=self.cfg.TRUST_REMOTE_CODE, device_map="auto", low_cpu_mem_usage=True
        )

        if self.cfg.USE_QLORA:
            model = prepare_model_for_kbit_training(model)

            # ====================[ LoRA è‡ªåŠ¨æ£€æµ‹ ]====================
            # å¦‚æœConfigä¸­æ²¡æœ‰æŒ‡å®šï¼Œåˆ™è‡ªåŠ¨æ£€æµ‹
            if not self.cfg.LORA_TARGET_MODULES:
                target_modules = self._find_lora_target_modules(model)
            else:
                print(f"ğŸ¯ ä½¿ç”¨Configä¸­æŒ‡å®šçš„LoRAç›®æ ‡æ¨¡å—: {self.cfg.LORA_TARGET_MODULES}")
                target_modules = self.cfg.LORA_TARGET_MODULES
            # ========================================================

            lora_config = LoraConfig(
                r=self.cfg.LORA_R, lora_alpha=self.cfg.LORA_ALPHA,
                target_modules=target_modules,  # <--- ä½¿ç”¨æ£€æµ‹åˆ°çš„æˆ–æŒ‡å®šçš„æ¨¡å—
                lora_dropout=self.cfg.LORA_DROPOUT, bias="none", task_type="CAUSAL_LM"
            )
            model = get_peft_model(model, lora_config)

        if self.cfg.USE_QLORA:
            model.print_trainable_parameters()

        return model, processor

    def _collate_fn(self, batch):
        batch = [item for item in batch if item is not None]
        if not batch: return None

        keys = batch[0].keys()
        padded_batch = {}
        for key in keys:
            values = [item[key] for item in batch]
            if key in ['pixel_values', 'image_grid_thw']:
                padded_batch[key] = torch.stack(values, dim=0)
            elif key in ['input_ids', 'attention_mask', 'labels']:
                tokenizer = self.processor.tokenizer if hasattr(self.processor, 'tokenizer') else self.processor
                padding_value = -100 if key == 'labels' else tokenizer.pad_token_id
                padded_batch[key] = torch.nn.utils.rnn.pad_sequence(values, batch_first=True,
                                                                    padding_value=padding_value)
        return padded_batch

    def train(self):
        dataset = UniversalDataset(self.cfg, self.processor)
        dataloader = DataLoader(dataset, batch_size=self.cfg.BATCH_SIZE, shuffle=True, collate_fn=self._collate_fn,
                                num_workers=2)

        total_steps = (len(dataloader) // self.cfg.GRAD_ACCUMULATION_STEPS) * self.cfg.EPOCHS
        optimizer = AdamW(self.model.parameters(), lr=self.cfg.LR)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=min(100, total_steps // 10),
                                                    num_training_steps=total_steps)

        print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
        self.model.train()
        for epoch in range(self.cfg.EPOCHS):
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{self.cfg.EPOCHS}")
            for i, batch in enumerate(progress_bar):
                if batch is None: continue

                batch = {k: v.to(self.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}

                outputs = self.model(**batch)
                loss = outputs.loss / self.cfg.GRAD_ACCUMULATION_STEPS
                loss.backward()

                if (i + 1) % self.cfg.GRAD_ACCUMULATION_STEPS == 0:
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()

                progress_bar.set_postfix({'loss': f'{loss.item() * self.cfg.GRAD_ACCUMULATION_STEPS:.4f}'})

        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        save_path = f"./logs/{self.cfg.EXP_ID}"
        self.model.save_pretrained(save_path)
        self.processor.save_pretrained(save_path)
        print(f"ğŸ’¾ æ¨¡å‹å’Œå¤„ç†å™¨å·²ä¿å­˜åˆ° {save_path}")

    def _find_lora_target_modules(self, model):
        """
        è‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰å¯åº”ç”¨LoRAçš„çº¿æ€§å±‚åç§°ã€‚
        """
        print("ğŸ¯ æ­£åœ¨è‡ªåŠ¨æ£€æµ‹LoRAç›®æ ‡æ¨¡å—...")
        lora_module_names = set()
        # æˆ‘ä»¬åªå…³å¿ƒå¸¸è§çš„Attentionå’ŒMLPå±‚åï¼Œä»¥æé«˜ç¨³å®šæ€§
        # å¯¹äºQwen2ç³»åˆ—ï¼Œå¸¸è§çš„çº¿æ€§å±‚åœ¨qkv_proj, o_proj, up_proj, gate_proj, down_proj
        supported_lora_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
            "qkv_proj"
        ]

        for name, module in model.named_modules():
            if isinstance(module, (torch.nn.Linear, bnb.nn.Linear4bit, bnb.nn.Linear8bitLt)):
                module_name = name.split('.')[-1]
                if module_name in supported_lora_modules:
                    lora_module_names.add(module_name)

        # é€šå¸¸ä¸å»ºè®®å¯¹è§†è§‰æ¨¡å‹çš„è¾“å‡ºæŠ•å½±å±‚å’Œè¯­è¨€æ¨¡å‹çš„è¾“å‡ºå±‚åº”ç”¨LoRA
        if 'lm_head' in lora_module_names:
            lora_module_names.remove('lm_head')

        print(f"âœ… è‡ªåŠ¨æŸ¥æ‰¾åˆ°çš„LoRAç›®æ ‡æ¨¡å—: {list(lora_module_names)}")
        return list(lora_module_names)


# ========================================================================================
# 4. æ‰§è¡Œå…¥å£
# ========================================================================================
if __name__ == "__main__":
    config = Config()
    trainer = Trainer(config)
    trainer.train()