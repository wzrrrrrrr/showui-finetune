#!/usr/bin/env python3
"""
ShowUI-2Bå¾®è°ƒè®­ç»ƒè„šæœ¬
æ”¯æŒNVIDIA GPU + CUDAï¼Œå…¼å®¹peftå’Œbitsandbytes
"""
import bitsandbytes as bnb
import argparse
import os
import sys
import json
from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import LinearLR
import transformers
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoProcessor, BitsAndBytesConfig
from tqdm import tqdm
from PIL import Image
from functools import partial


class ShowUIDataset(Dataset):
    """ShowUIæ•°æ®é›†ç±»"""

    def __init__(self, data_path, processor, args):
        self.data_path = data_path
        self.processor = processor
        self.args = args

        # å°†ç³»ç»Ÿæç¤ºå®šä¹‰ä¸ºç±»çš„å±æ€§
        self.system_prompt = "Based on the screenshot of the page, I give a text description and you give its corresponding location. The coordinate represents a clickable location [x, y] for an element, which is a relative coordinate on the screenshot, scaled from 0 to 1."

        # è¯»å–æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            if data_path.endswith('.json'):
                # JSONæ ¼å¼
                self.data = json.load(f)
            else:
                # JSONLæ ¼å¼
                self.data = []
                for line in f:
                    if line.strip():
                        self.data.append(json.loads(line))

        print(f"ğŸ“Š åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = "æœªå®šä¹‰"  # åˆå§‹åŒ–

        try:
            # 1. æå–ä¿¡æ¯ (è¿™éƒ¨åˆ†ä¸å˜)
            image_filename = item['img_url']
            image_path = os.path.join(os.path.dirname(self.data_path), 'images', image_filename)
            element = item['element'][0]
            instruction = element.get('instruction', 'ç›®æ ‡åŒºåŸŸ')
            point = element['point']

            # 2. æ„å»º messages åˆ—è¡¨ (ç”¨äºç”Ÿæˆæ–‡æœ¬æ¨¡æ¿)
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.system_prompt},
                        {"type": "image"},  # è¿™é‡Œåªéœ€è¦ä¸€ä¸ªå ä½ç¬¦
                        {"type": "text", "text": instruction}
                    ]
                },
                {
                    "role": "assistant",
                    "content": str(point)
                }
            ]

            # ================ [ å…¨æ–°ã€æœ€å…³é”®çš„ä¿®æ”¹ ] ================
            # ä¸¥æ ¼éµå¾ªå®˜æ–¹æ–‡æ¡£çš„â€œæ‰‹åŠ¨ä¸‰æ­¥æ³•â€

            # æ­¥éª¤ A: åƒå®˜æ–¹ä¸€æ ·ï¼Œç”¨ apply_chat_template åªç”Ÿæˆæ–‡æœ¬éƒ¨åˆ†
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥ç”¨ tokenizer çš„æ¨¡æ¿åŠŸèƒ½ï¼Œæ›´åº•å±‚ä¹Ÿæ›´ç¨³å®š
            text = self.processor.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False  # è®­ç»ƒæ—¶è®¾ä¸ºFalseï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ é¢„æµ‹<|im_start|>assistant
            )

            # æ­¥éª¤ B: æ‰‹åŠ¨æ¨¡æ‹Ÿ process_vision_info çš„æ ¸å¿ƒåŠŸèƒ½
            #   B.1 åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            #   B.2 ä½¿ç”¨ processor å†…éƒ¨çš„ image_processor å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†ï¼Œå¾—åˆ°å›¾ç‰‡å¼ é‡
            #       è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰æ‰€æœ‰æ–¹æ¡ˆéƒ½ç¼ºå¤±çš„æœ€å…³é”®ä¸€æ­¥ï¼
            image_inputs_dict = self.processor.image_processor(images=image, return_tensors="pt")

            # æ­¥éª¤ C: å°†æœ€ç»ˆçš„æ–‡æœ¬å’Œå›¾ç‰‡å ä½ç¬¦åˆå¹¶ï¼Œè¿›è¡Œæœ€åå¤„ç†
            #       è¿™é‡Œæˆ‘ä»¬åªç”¨ tokenizerï¼Œå› ä¸ºå®ƒè´Ÿè´£å°†æ–‡æœ¬å’Œè§†è§‰å ä½ç¬¦åˆå¹¶
            inputs = self.processor.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=self.args.model_max_length
                # padding åœ¨ collate_fn ä¸­å¤„ç†
            )

            # æ­¥éª¤ D: å°†é¢„å¤„ç†å¥½çš„å›¾ç‰‡å¼ é‡æ·»åŠ åˆ° inputs å­—å…¸ä¸­
            # Qwen-VLæ¨¡å‹æœŸæœ›è¿™ä¸ªé”®åä¸º 'pixel_values'
            inputs['pixel_values'] = image_inputs
            inputs['image_grid_thw'] = image_inputs_dict['image_grid_thw']
            # ================== [ ä¿®æ”¹ç»“æŸ ] ==================

            # 5. åç»­å¤„ç† (è¿™éƒ¨åˆ†ä¸å˜)
            inputs["labels"] = inputs["input_ids"].clone()
            # ä» PyTorch å¼ é‡ä¸­ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå› ä¸ºDataLoaderä¼šè‡ªåŠ¨æ·»åŠ 
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor) and inputs[key].dim() > 1:
                    inputs[key] = inputs[key].squeeze(0)

            return inputs

        except Exception as e:
            # é”™è¯¯å¤„ç†é€»è¾‘ä¿æŒä¸å˜
            import traceback
            print(f"âŒ å¤„ç†æ•°æ®æ—¶å‡ºé”™! Item index: {idx}, å°è¯•çš„å›¾ç‰‡è·¯å¾„: {image_path}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {e}")
            traceback.print_exc()

            # è¿”å›ä¸€ä¸ªNoneï¼Œè®©collate_fnå¯ä»¥è¿‡æ»¤æ‰å®ƒ
            return None


def parse_args():
    parser = argparse.ArgumentParser(description="ShowUI-2Bå¾®è°ƒè®­ç»ƒ")

    # åŸºç¡€å‚æ•°
    parser.add_argument("--precision", default="bf16", type=str, choices=["fp32", "bf16", "fp16"])
    parser.add_argument("--use_qlora", action="store_true", default=True)
    parser.add_argument("--load_in_4bit", action="store_true", default=True)

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_id", default="showlab/ShowUI-2B")
    parser.add_argument("--local_weight_dir", default="./models", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")

    # æ•°æ®å‚æ•°
    parser.add_argument("--dataset_dir", default="./data", type=str)
    parser.add_argument("--train_json", default="my_dataset/metadata.json", type=str)
    parser.add_argument("--model_max_length", default=2048, type=int)

    # LoRAå‚æ•°
    parser.add_argument("--lora_r", default=16, type=int)
    parser.add_argument("--lora_alpha", default=32, type=int)
    parser.add_argument("--lora_dropout", default=0.1, type=float)

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--log_base_dir", default="./logs", type=str)
    parser.add_argument("--exp_id", default="showui_finetune", type=str)
    parser.add_argument("--epochs", default=3, type=int)
    parser.add_argument("--max_steps", default=None, type=int, help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--lr", default=2e-4, type=float)
    parser.add_argument("--batch_size", default=1, type=int)
    parser.add_argument("--grad_accumulation_steps", default=8, type=int)
    parser.add_argument("--warmup_steps", default=100, type=int)
    parser.add_argument("--print_freq", default=10, type=int)
    parser.add_argument("--save_steps", default=500, type=int)

    return parser.parse_args()


def setup_model_and_processor(args):
    """è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨"""
    print("ğŸ”§ æ­£åœ¨è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨...")

    # ç¡®å®šè®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸš€ ä½¿ç”¨CUDA: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨CPU")

    # è®¾ç½®æ•°æ®ç±»å‹
    torch_dtype = torch.float32
    if args.precision == "bf16":
        torch_dtype = torch.bfloat16
    elif args.precision == "fp16":
        torch_dtype = torch.half

    # é‡åŒ–é…ç½®
    bnb_config = None
    if args.use_qlora and args.load_in_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch_dtype,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

    model_path = os.path.join(args.local_weight_dir, args.model_id.split('/')[-1])

    # ==================== [ å…¨æ–°ã€æœ€å…³é”®çš„ä¿®æ”¹ ] ====================
    # åŠ è½½å¤„ç†å™¨
    try:
        print(f"ğŸ”§ æ­£åœ¨ä» '{model_path}' åŠ è½½å¤„ç†å™¨...")

        # æ ¹æ®å®˜æ–¹æ–‡æ¡£å’Œä»£ç ï¼Œè®¡ç®—åƒç´ å€¼
        # min_visual_tokens = 256, max_visual_tokens = 1344
        # æ¯ä¸ªvisual tokenå¯¹åº”ä¸€ä¸ª 28x28 çš„patch
        min_pixels = 256 * 28 * 28  # 200704
        max_pixels = 1344 * 28 * 28  # 1053696

        processor = AutoProcessor.from_pretrained(
            model_path,
            trust_remote_code=True,
            # ä¼ å…¥è¿™ä¸¤ä¸ªå…³é”®å‚æ•°ï¼Œç¡®ä¿image_processorèƒ½æ­£ç¡®å¤„ç†ä¸åŒå°ºå¯¸çš„å›¾ç‰‡
            min_pixels=min_pixels,
            max_pixels=max_pixels,
            size = {'shortest_edge' : 448, 'longest_edge' : 448}
        )
        print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸï¼Œå¹¶å·²è®¾ç½® min/max_pixelsã€‚")

        # è®¾ç½®èŠå¤©æ¨¡æ¿ (è¿™éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œåšå¾—å¾ˆå¥½)
        CHAT_TEMPLATE = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
        processor.chat_template = CHAT_TEMPLATE
        processor.tokenizer.chat_template = CHAT_TEMPLATE
        print("ğŸ‘ å·²æˆåŠŸè®¾ç½®å®˜æ–¹ ShowUI èŠå¤©æ¨¡æ¿")

    except Exception as e:
        print(f"âŒ å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
        return None, None, None
    # ==================== [ ä¿®æ”¹ç»“æŸ ] ====================

    # åŠ è½½æ¨¡å‹
    try:
        from transformers import Qwen2VLForConditionalGeneration

        print(f"ğŸ”§ æ­£åœ¨ä» '{model_path}' åŠ è½½æ¨¡å‹...")
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch_dtype,
            device_map="auto",
            quantization_config=bnb_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿ShowUI-2Bæ¨¡å‹æ–‡ä»¶åœ¨ {model_path} ç›®å½•ä¸‹")
        return None, None, None

    return model, processor, device


def setup_lora(model, args):
    """è®¾ç½®LoRAå¾®è°ƒ"""
    if args.lora_r <= 0:
        return model

    print("ğŸ”§ æ­£åœ¨è®¾ç½®LoRA...")

    if args.use_qlora:
        model = prepare_model_for_kbit_training(model)

    target_modules = find_all_linear_names(model)
    print(f"ğŸ¯ è‡ªåŠ¨æŸ¥æ‰¾åˆ°çš„LoRAç›®æ ‡æ¨¡å—: {target_modules}")

    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=target_modules,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    return model


def find_all_linear_names(model):
    """
    è‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰å¯åº”ç”¨LoRAçš„çº¿æ€§å±‚åç§°ã€‚
    è¿™ä¸ªå®ç°å¾ˆå¥½ï¼Œæ—¢é€šç”¨åˆå®‰å…¨ã€‚
    """
    supported_lora_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "qkv_proj"
    ]

    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt, torch.nn.Linear)):
            module_name = name.split('.')[-1]
            if module_name in supported_lora_modules:
                lora_module_names.add(module_name)

    # ä¸å¯¹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºå±‚åº”ç”¨LoRAï¼Œè¿™æ˜¯ä¸€ç§å¸¸è§çš„ã€èƒ½æé«˜ç¨³å®šæ€§çš„åšæ³•
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')

    # ShowUIä¸­æ²¡æœ‰è¿™äº›ï¼Œä½†ä¿ç•™æ˜¯å¥½çš„å®è·µ
    if 'proj' in lora_module_names:
        lora_module_names.remove('proj')

    return list(lora_module_names)


def main():
    def collate_fn(batch, processor):
        # è¿‡æ»¤æ‰å› ä¸ºè¯»å–é”™è¯¯ç­‰åŸå› è¿”å›çš„ None æ ·æœ¬
        batch = [item for item in batch if item is not None]
        if not batch:
            return None

        # æ‚¨çš„collate_fnå®ç°å¾—å¾ˆå¥½ï¼Œå¯ä»¥ä¿æŒåŸæ ·
        try:
            keys = batch[0].keys()
            padded_batch = {}

            for key in keys:
                values = [item[key] for item in batch]

                if key == 'pixel_values':
                    # pixel_values éƒ½æ˜¯ç›¸åŒå°ºå¯¸çš„ï¼Œç›´æ¥ç”¨ stack åˆå¹¶
                    padded_batch[key] = torch.stack(values, dim=0)
                elif key in ['input_ids', 'attention_mask', 'labels']:
                    # æ–‡æœ¬ç›¸å…³å¼ é‡éœ€è¦å¡«å……åˆ°æ‰¹å†…æœ€å¤§é•¿åº¦
                    padding_value = -100 if key == 'labels' else processor.tokenizer.pad_token_id

                    # ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ pad_sequence è¿›è¡Œå¡«å……ï¼Œéå¸¸é«˜æ•ˆ
                    padded_batch[key] = torch.nn.utils.rnn.pad_sequence(
                        values, batch_first=True, padding_value=padding_value
                    )
                else:
                    padded_batch[key] = values

            return padded_batch

        except Exception as e:
            import traceback
            print(f"âŒ collate_fn ä¸­å‡ºé”™!")
            traceback.print_exc()
            return None

    args = parse_args()

    print("ğŸš€ å¼€å§‹ShowUI-2Bå¾®è°ƒè®­ç»ƒ")
    print(f"ğŸ“± æ¨¡å‹: {args.model_id}")
    print(f"ğŸ¯ å®éªŒ: {args.exp_id}")

    # è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨
    model, processor, device = setup_model_and_processor(args)
    if model is None:
        print("âŒ æ¨¡å‹è®¾ç½®å¤±è´¥ï¼Œé€€å‡º")
        return

    # è®¾ç½®LoRA
    model = setup_lora(model, args)

    # åˆ›å»ºæ•°æ®é›†
    train_data_path = os.path.join(args.dataset_dir, args.train_json)
    dataset = ShowUIDataset(train_data_path, processor, args)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4,
                            collate_fn=partial(collate_fn, processor=processor))

    # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=args.lr)

    # è®¡ç®—æ€»æ­¥æ•°
    num_update_steps_per_epoch = len(dataloader) // args.grad_accumulation_steps
    if args.max_steps is None:
        total_steps = args.epochs * num_update_steps_per_epoch
    else:
        total_steps = args.max_steps

    scheduler = transformers.get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=total_steps
    )

    # è®­ç»ƒå¾ªç¯
    print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    global_step = 0
    model.train()

    for epoch in range(args.epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{args.epochs}", total=len(dataloader))

        for step, batch in enumerate(progress_bar):
            if batch is None:
                continue

            if args.max_steps and global_step >= args.max_steps:
                print(f"ğŸ¯ è¾¾åˆ°æœ€å¤§æ­¥æ•° {args.max_steps}ï¼Œåœæ­¢è®­ç»ƒ")
                break

            try:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = {k: v.to(device) for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs.loss

                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / args.grad_accumulation_steps

                # åå‘ä¼ æ’­
                loss.backward()

                if (step + 1) % args.grad_accumulation_steps == 0:
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1

                    # ä»…åœ¨å®é™…æ›´æ–°åè®°å½•æ—¥å¿—
                    current_lr = scheduler.get_last_lr()[0]
                    progress_bar.set_postfix({
                        'loss': f'{loss.item() * args.grad_accumulation_steps:.4f}',
                        'lr': f'{current_lr:.2e}',
                        'step': global_step
                    })

            except Exception as e:
                print(f"âš ï¸ è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                # æ¸…ç©ºæ¢¯åº¦ä»¥é˜²ä¸‡ä¸€
                optimizer.zero_grad()
                continue

        # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œæå‰ç»“æŸå¤–å±‚å¾ªç¯
        if args.max_steps and global_step >= args.max_steps:
            break

    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(args.log_base_dir, args.exp_id, f"checkpoint-final")
    os.makedirs(save_path, exist_ok=True)
    model.save_pretrained(save_path)
    processor.save_pretrained(save_path)
    print(f"ğŸ’¾ æ¨¡å‹å’Œå¤„ç†å™¨å·²ä¿å­˜åˆ° {save_path}")


if __name__ == "__main__":
    main()