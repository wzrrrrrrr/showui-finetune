#!/usr/bin/env python3
"""
ShowUI-2Bå¾®è°ƒè®­ç»ƒè„šæœ¬
æ”¯æŒNVIDIA GPU + CUDAï¼Œå…¼å®¹peftå’Œbitsandbytes
"""
import bitsandbytes as bnb
import argparse
import os
import sys
import json
from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import LinearLR
import transformers
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoProcessor, BitsAndBytesConfig
from tqdm import tqdm
from PIL import Image
from functools import partial  # <--- åœ¨è¿™é‡ŒåŠ ä¸Šè¿™è¡Œ


class ShowUIDataset(Dataset):
    """ShowUIæ•°æ®é›†ç±»"""

    def __init__(self, data_path, processor, args):
        self.data_path = data_path
        self.processor = processor
        self.args = args

        # å°†ç³»ç»Ÿæç¤ºå®šä¹‰ä¸ºç±»çš„å±æ€§
        self.system_prompt = "Based on the screenshot of the page, I give a text description and you give its corresponding location. The coordinate represents a clickable location [x, y] for an element, which is a relative coordinate on the screenshot, scaled from 0 to 1."

        # è¯»å–æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            if data_path.endswith('.json'):
                # JSONæ ¼å¼
                self.data = json.load(f)
            else:
                # JSONLæ ¼å¼
                self.data = []
                for line in f:
                    if line.strip():
                        self.data.append(json.loads(line))

        print(f"ğŸ“Š åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")

    def __len__(self):
        return len(self.data)

    # åœ¨ä½ çš„ ShowUIDataset ç±»ä¸­
    # åœ¨ä½ çš„ ShowUIDataset ç±»ä¸­
    # åœ¨ä½ çš„ ShowUIDataset ç±»ä¸­
    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = "æœªå®šä¹‰"  # åˆå§‹åŒ–

        try:
            # 1. æå–ä¿¡æ¯ (è¿™éƒ¨åˆ†ä¸å˜)
            image_filename = item['img_url']
            image_path = os.path.join(os.path.dirname(self.data_path), 'images', image_filename)
            element = item['element'][0]
            instruction = element.get('instruction', 'ç›®æ ‡åŒºåŸŸ')
            point = element['point']

            # 2. æ„å»º messages åˆ—è¡¨ (ç”¨äºç”Ÿæˆæ–‡æœ¬æ¨¡æ¿)
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.system_prompt},
                        {"type": "image"},  # è¿™é‡Œåªéœ€è¦ä¸€ä¸ªå ä½ç¬¦
                        {"type": "text", "text": instruction}
                    ]
                },
                {
                    "role": "assistant",
                    "content": str(point)
                }
            ]

            # ================ [ å…¨æ–°ã€æœ€å…³é”®çš„ä¿®æ”¹ ] ================
            # ä¸¥æ ¼éµå¾ªå®˜æ–¹æ–‡æ¡£çš„â€œæ‰‹åŠ¨ä¸‰æ­¥æ³•â€

            # æ­¥éª¤ A: åƒå®˜æ–¹ä¸€æ ·ï¼Œç”¨ apply_chat_template åªç”Ÿæˆæ–‡æœ¬éƒ¨åˆ†
            text = self.processor.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )

            # æ­¥éª¤ B: æ‰‹åŠ¨æ¨¡æ‹Ÿ process_vision_info çš„æ ¸å¿ƒåŠŸèƒ½
            #   B.1 åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            #   B.2 ä½¿ç”¨ processor å†…éƒ¨çš„ image_processor å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†ï¼Œå¾—åˆ°å›¾ç‰‡å¼ é‡
            #       è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰æ‰€æœ‰æ–¹æ¡ˆéƒ½ç¼ºå¤±çš„æœ€å…³é”®ä¸€æ­¥ï¼
            image_inputs = self.processor.image_processor([image], return_tensors="pt")['pixel_values']

            # æ­¥éª¤ C: å°†æœ€ç»ˆçš„æ–‡æœ¬å’Œå›¾ç‰‡å¼ é‡ä¸€èµ·é€å…¥ tokenizer è¿›è¡Œæœ€åå¤„ç†
            #       è¿™é‡Œæˆ‘ä»¬åªç”¨ tokenizerï¼Œå› ä¸ºå®ƒè´Ÿè´£å°†æ–‡æœ¬å’Œè§†è§‰å ä½ç¬¦åˆå¹¶
            inputs = self.processor.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=self.args.model_max_length
                # padding åœ¨ collate_fn ä¸­å¤„ç†
            )

            # æ­¥éª¤ D: å°†é¢„å¤„ç†å¥½çš„å›¾ç‰‡å¼ é‡æ·»åŠ åˆ° inputs å­—å…¸ä¸­
            inputs['pixel_values'] = image_inputs

            # ================== [ ä¿®æ”¹ç»“æŸ ] ==================

            # 5. åç»­å¤„ç† (è¿™éƒ¨åˆ†ä¸å˜)
            inputs["labels"] = inputs["input_ids"].clone()
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor) and inputs[key].dim() > 1:
                    inputs[key] = inputs[key].squeeze(0)

            return inputs

        except Exception as e:
            # é”™è¯¯å¤„ç†é€»è¾‘ä¿æŒä¸å˜
            import traceback
            print(f"âŒ å¤„ç†æ•°æ®æ—¶å‡ºé”™! å°è¯•çš„å›¾ç‰‡è·¯å¾„: {image_path}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {e}")
            traceback.print_exc()  # æ‰“å°å®Œæ•´çš„å †æ ˆï¼Œå¸®åŠ©æˆ‘ä»¬çœ‹åˆ°åº•æ˜¯å“ªä¸€æ­¥é”™äº†

            # 2. è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…è¦é”®çš„ã€å®Œæ•´çš„ fallback å­—å…¸
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ pixel_values å¼ é‡
            dummy_pixel_values = torch.zeros((3, 448, 448), dtype=torch.float)
            dummy_input_ids = torch.zeros(100, dtype=torch.long)

            return {
                "pixel_values": dummy_pixel_values,
                "input_ids": dummy_input_ids,
                "attention_mask": torch.ones_like(dummy_input_ids),
                "labels": torch.full_like(dummy_input_ids, -100)  # æ ‡ç­¾ç”¨-100å¡«å……
            }
        # ================ [ ä¿®æ”¹ç»“æŸ ] ================

def parse_args():
    parser = argparse.ArgumentParser(description="ShowUI-2Bå¾®è°ƒè®­ç»ƒ")

    # åŸºç¡€å‚æ•°
    parser.add_argument("--precision", default="bf16", type=str, choices=["fp32", "bf16", "fp16"])
    parser.add_argument("--use_qlora", action="store_true", default=True)
    parser.add_argument("--load_in_4bit", action="store_true", default=True)
    parser.add_argument("--use_text_only", action="store_true", default=False, help="ä»…ä½¿ç”¨æ–‡æœ¬è®­ç»ƒ")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_id", default="showlab/ShowUI-2B")
    parser.add_argument("--local_weight", action="store_true", default=True)
    parser.add_argument("--local_weight_dir", default="./models", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")

    # æ•°æ®å‚æ•°
    parser.add_argument("--dataset_dir", default="./data", type=str)
    parser.add_argument("--train_json", default="my_dataset/metadata.json", type=str)
    parser.add_argument("--model_max_length", default=2048, type=int)

    # LoRAå‚æ•°
    parser.add_argument("--lora_r", default=16, type=int)
    parser.add_argument("--lora_alpha", default=32, type=int)
    parser.add_argument("--lora_dropout", default=0.1, type=float)

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--log_base_dir", default="./logs", type=str)
    parser.add_argument("--exp_id", default="showui", type=str)
    parser.add_argument("--epochs", default=3, type=int)
    parser.add_argument("--max_steps", default=None, type=int, help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--lr", default=2e-4, type=float)
    parser.add_argument("--batch_size", default=1, type=int)
    parser.add_argument("--grad_accumulation_steps", default=8, type=int)
    parser.add_argument("--warmup_steps", default=100, type=int)
    parser.add_argument("--print_freq", default=10, type=int)
    parser.add_argument("--save_steps", default=500, type=int)

    return parser.parse_args()


def setup_model_and_processor(args):
    """è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨"""
    print("ğŸ”§ æ­£åœ¨è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨...")

    # ç¡®å®šè®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸš€ ä½¿ç”¨CUDA: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨CPU")

    # è®¾ç½®æ•°æ®ç±»å‹
    torch_dtype = torch.float32
    if args.precision == "bf16":
        torch_dtype = torch.bfloat16
    elif args.precision == "fp16":
        torch_dtype = torch.half

    # é‡åŒ–é…ç½®
    bnb_config = None
    if args.use_qlora and args.load_in_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch_dtype,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

    # åŠ è½½å¤„ç†å™¨
    try:
        print(f"ğŸ”§ æ­£åœ¨ä» '{args.model_id}' åŠ è½½å¤„ç†å™¨...")

        # å®šä¹‰ Qwen2-VL ä¸“ç”¨çš„å°ºå¯¸å‚æ•°
        min_pixels = 256 * 28 * 28
        max_pixels = 1344 * 28 * 28

        processor = AutoProcessor.from_pretrained(
            args.model_id,
            min_pixels=min_pixels,
            max_pixels=max_pixels,
            trust_remote_code=True
        )
        print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸï¼Œå¹¶å·²ä½¿ç”¨æ­£ç¡®çš„ min/max_pixels é…ç½®ï¼")
        # size = {"shortest_edge": 448, "longest_edge": 448},

        CHAT_TEMPLATE = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
        processor.chat_template = CHAT_TEMPLATE
        processor.tokenizer.chat_template = CHAT_TEMPLATE
        print("ğŸ‘ å·²æˆåŠŸè®¾ç½®å®˜æ–¹ ShowUI èŠå¤©æ¨¡æ¿")

    except Exception as e:
        print(f"âŒ å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
        return None, None, None

    # åŠ è½½æ¨¡å‹
    try:
        from transformers import Qwen2VLForConditionalGeneration

        model_path = f"{args.local_weight_dir}/ShowUI-2B"

        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch_dtype,
            device_map="auto",
            quantization_config=bnb_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ShowUI-2Bæ¨¡å‹æ–‡ä»¶åœ¨ ./models/ShowUI-2B ç›®å½•ä¸‹")
        return None, None, None

    return model, processor, device


def setup_lora(model, args):
    """è®¾ç½®LoRAå¾®è°ƒ"""
    if args.lora_r <= 0:
        return model

    print("ğŸ”§ æ­£åœ¨è®¾ç½®LoRA...")

    if args.use_qlora:
        model = prepare_model_for_kbit_training(model)

    target_modules = find_all_linear_names(model)
    print(f"ğŸ¯ è‡ªåŠ¨æŸ¥æ‰¾åˆ°çš„LoRAç›®æ ‡æ¨¡å—: {target_modules}")

    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=target_modules,  # <--- ä½¿ç”¨è‡ªåŠ¨æŸ¥æ‰¾åˆ°çš„åˆ—è¡¨
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    return model


def find_all_linear_names(model):
    """
    è‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰å¯åº”ç”¨LoRAçš„çº¿æ€§å±‚åç§°ã€‚
    è¿™æ¬¡çš„å®ç°æ›´å®‰å…¨ï¼Œåªè€ƒè™‘äº†å¸¸è§çš„Attentionå’ŒMLPå±‚åã€‚
    """
    # ç›®æ ‡æ¨¡å—çš„å¸¸è§åç§°
    # å¯¹äºQwen2ç³»åˆ—ï¼Œå¸¸è§çš„çº¿æ€§å±‚åœ¨qkv_proj, o_proj, up_proj, gate_proj, down_proj
    # æˆ‘ä»¬è¿™é‡Œåˆ—ä¸€ä¸ªæ›´é€šç”¨çš„åˆ—è¡¨
    supported_lora_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "qkv_proj", "out_proj", "in_proj",  # é€‚ç”¨äºå…¶ä»–æ¨¡å‹çš„åå­—
        "fc1", "fc2"  # Vision Transformer ä¸­çš„åå­—
    ]

    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt, torch.nn.Linear)):
            # è·å–æ¨¡å—åçš„æœ€åä¸€éƒ¨åˆ†
            module_name = name.split('.')[-1]
            # åªæœ‰å½“è¿™ä¸ªåå­—åœ¨æˆ‘ä»¬æ”¯æŒçš„åˆ—è¡¨ä¸­æ—¶ï¼Œæ‰æ·»åŠ å®ƒ
            if module_name in supported_lora_modules:
                lora_module_names.add(module_name)

    # ä¸å¯¹è§†è§‰ç¼–ç å™¨çš„æŠ•å½±å±‚å’Œè¯­è¨€æ¨¡å‹çš„è¾“å‡ºå±‚åº”ç”¨LoRA
    # è¿™æ˜¯ä¸€ç§å¸¸è§çš„ã€èƒ½æé«˜ç¨³å®šæ€§çš„åšæ³•
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    if 'proj' in lora_module_names:
        lora_module_names.remove('proj')  # é€šå¸¸æ˜¯ViTçš„è¾“å‡ºæŠ•å½±ï¼Œä¸å»ºè®®LoRA

    return list(lora_module_names)


def main():
    # é—®é¢˜: å½“å‰çš„ padding æ˜¯åœ¨ __getitem__ ä¸­é€šè¿‡ padding="max_length" å®ç°çš„ã€‚è¿™æ„å‘³ç€æ¯ä¸ªæ ·æœ¬éƒ½ä¼šè¢«å¡«å……åˆ° model_max_lengthï¼Œå¯èƒ½ä¼šæµªè´¹å¤§é‡æ˜¾å­˜å’Œè®¡ç®—ã€‚
    # å»ºè®®: ä½¿ç”¨åŠ¨æ€æ‰¹å¤„ç†å¡«å……ï¼ˆDynamic Paddingï¼‰ã€‚è¿™éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ª collate_fnã€‚
    def collate_fn(batch, processor):
        # å°†æ‰¹æ¬¡ä¸­çš„æ ·æœ¬è§£æ„
        pixel_values = torch.cat([item['pixel_values'] for item in batch], dim=0)

        # å¯¹æ–‡æœ¬éƒ¨åˆ†è¿›è¡ŒåŠ¨æ€å¡«å……
        text_inputs = processor.tokenizer.pad(
            [{"input_ids": item["input_ids"], "attention_mask": item["attention_mask"]} for item in batch],
            return_tensors="pt",
            padding=True
        )

        # å¯¹æ ‡ç­¾ä¹Ÿè¿›è¡Œå¡«å……ï¼Œä½¿ç”¨ -100 å¿½ç•¥ padding éƒ¨åˆ†çš„æŸå¤±
        labels = processor.tokenizer.pad(
            [{"input_ids": item["labels"]} for item in batch],
            return_tensors="pt",
            padding=True
        )["input_ids"]
        labels[labels == processor.tokenizer.pad_token_id] = -100

        return {
            'pixel_values': pixel_values,
            'input_ids': text_inputs['input_ids'],
            'attention_mask': text_inputs['attention_mask'],
            'labels': labels
        }

    args = parse_args()

    print("ğŸš€ å¼€å§‹ShowUI-2Bå¾®è°ƒè®­ç»ƒ")
    print(f"ğŸ“± æ¨¡å‹: {args.model_id}")
    print(f"ğŸ¯ å®éªŒ: {args.exp_id}")

    # è®¾ç½®æ¨¡å‹å’Œå¤„ç†å™¨
    model, processor, device = setup_model_and_processor(args)
    if model is None:
        print("âŒ æ¨¡å‹è®¾ç½®å¤±è´¥ï¼Œé€€å‡º")
        return

    # è®¾ç½®LoRA
    model = setup_lora(model, args)

    # åˆ›å»ºæ•°æ®é›†
    train_data_path = os.path.join(args.dataset_dir, args.train_json)
    dataset = ShowUIDataset(train_data_path, processor, args)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4,
                            collate_fn=partial(collate_fn, processor=processor))

    # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=args.lr)
    total_steps = args.epochs * len(dataloader) // args.grad_accumulation_steps
    if args.max_steps:
        total_steps = min(total_steps, args.max_steps)

    scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=args.warmup_steps)

    # è®­ç»ƒå¾ªç¯
    print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    global_step = 0

    for epoch in range(args.epochs):
        model.train()
        total_loss = 0

        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{args.epochs}")

        for step, batch in enumerate(progress_bar):
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
            if args.max_steps and global_step >= args.max_steps:
                print(f"ğŸ¯ è¾¾åˆ°æœ€å¤§æ­¥æ•° {args.max_steps}ï¼Œåœæ­¢è®­ç»ƒ")
                break

            try:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                if device.type != "cpu":
                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                             for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs.loss / args.grad_accumulation_steps

                # åå‘ä¼ æ’­
                loss.backward()

                if (step + 1) % args.grad_accumulation_steps == 0:
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1

                total_loss += loss.item() * args.grad_accumulation_steps

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'loss': f'{loss.item() * args.grad_accumulation_steps:.4f}',
                    'lr': f'{scheduler.get_last_lr()[0]:.2e}',
                    'step': f'{global_step}'
                })

            except Exception as e:
                print(f"âš ï¸ è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
                continue

        avg_loss = total_loss / max(len(dataloader), 1)
        print(f"Epoch {epoch + 1}/{args.epochs} - å¹³å‡æŸå¤±: {avg_loss:.4f} - æ€»æ­¥æ•°: {global_step}")

        # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œæå‰ç»“æŸ
        if args.max_steps and global_step >= args.max_steps:
            break

    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")

    # ä¿å­˜æ¨¡å‹
    save_path = f"{args.log_base_dir}/{args.exp_id}"
    os.makedirs(save_path, exist_ok=True)
    model.save_pretrained(save_path)
    print(f"ğŸ’¾ æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ° {save_path}")


if __name__ == "__main__":
    main()
